
@article{albers_problem_2019,
  title = {The Problem with Unadjusted Multiple and Sequential Statistical Testing},
  author = {Albers, Casper},
  year = {2019},
  month = apr,
  volume = {10},
  pages = {1921},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09941-0},
  abstract = {In research studies, the need for additional samples to obtain sufficient statistical power has often to be balanced with the experimental costs. One approach to this end is to sequentially collect data until you have sufficient measurements, e.g., when the p-value drops below 0.05. I outline that this approach is common, yet that unadjusted sequential sampling leads to severe statistical issues, such as an inflated rate of false positive findings. As a consequence, the results of such studies are untrustworthy. I identify the statistical methods that can be implemented in order to account for sequential sampling.},
  copyright = {2019 The Author(s)},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{armitage_repeated_1969,
	title = {Repeated significance tests on accumulating data},
	volume = {132},
	doi = {https://doi.org/10.2307/2343787},
	abstract = {If significance tests at a fixed level are repeated at stages during the accumulation of data the probability of obtaining a significant result when the null hypothesis is true rises above the nominal significance level. Numerical results are presented for repeated tests on cumulative series of binomial, normal and exponential observations.},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (General)},
	author = {Armitage, Peter and McPherson, C. K. and Rowe, B. C.},
	year = {1969},
	note = {Publisher: Wiley Online Library},
	pages = {235--244},
	file = {Armitage et al_1969_Repeated significance tests on accumulating data.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\UQHSHG5M\\Armitage et al_1969_Repeated significance tests on accumulating data.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\7KSRIUNV\\2343787.html:text/html}
}

@book{cohen_statistical_1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  edition = {2nd ed},
  publisher = {{L. Erlbaum Associates}},
  address = {{Hillsdale, N.J}},
  isbn = {978-0-8058-0283-2},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  lccn = {HA29 .C66 1988}
}

@article{cook_p-value_2002,
	title = {P-value adjustment in sequential clinical trials},
	volume = {58},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2002.01005.x},
	doi = {https://doi.org/10.1111/j.0006-341X.2002.01005.x},
	abstract = {Summary. Many randomized clinical trials utilize a group sequential monitoring procedure for assessing treatment efficacy during the course of the trial. For a group sequential trial in which the null hypothesis is ultimately rejected, the nominal p-value generally overstates the statistical significance of the result and some adjustment is required. Several orderings of the sample space have been proposed for performing this adjustment, each with unique operating characteristics. In this article, we compare four proposed methods with respect to the degree to which each captures the strength of evidence against the null hypothesis implied by the data. We conclude that the ordering of the sample space induced by the observed z-score has the most desirable operating characteristics.},
	language = {en},
	number = {4},
	urldate = {2021-01-12},
	journal = {Biometrics},
	author = {Cook, Thomas D.},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2002.01005.x},
	keywords = {Group sequential clinical trials, Likelihood principle, P-value adjustment},
	pages = {1005--1011},
	file = {Cook_2002_P-Value Adjustment in Sequential Clinical Trials.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\VGZRJZFE\\Cook_2002_P-Value Adjustment in Sequential Clinical Trials.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\DAUKVIFA\\j.0006-341X.2002.01005.html:text/html}
}

@article{dimairo_adaptive_2020,
  title = {The {{Adaptive}} Designs {{CONSORT Extension}} ({{ACE}}) Statement: A Checklist with Explanation and Elaboration Guideline for Reporting Randomised Trials That Use an Adaptive Design},
  shorttitle = {The {{Adaptive}} Designs {{CONSORT Extension}} ({{ACE}}) Statement},
  author = {Dimairo, Munyaradzi and Pallmann, Philip and Wason, James and Todd, Susan and Jaki, Thomas and Julious, Steven A. and Mander, Adrian P. and Weir, Christopher J. and Koenig, Franz and Walton, Marc K. and Nicholl, Jon P. and Coates, Elizabeth and Biggs, Katie and Hamasaki, Toshimitsu and Proschan, Michael A. and Scott, John A. and Ando, Yuki and Hind, Daniel and Altman, Douglas G. and {ACE Consensus Group}},
  year = {2020},
  month = jun,
  volume = {369},
  pages = {m115},
  issn = {1756-1833},
  doi = {10.1136/bmj.m115},
  abstract = {Adaptive designs (ADs) allow pre-planned changes to an ongoing trial without compromising the validity of conclusions and it is essential to distinguish pre-planned from unplanned changes that may also occur. The reporting of ADs in randomised trials is inconsistent and needs improving. Incompletely reported AD randomised trials are difficult to reproduce and are hard to interpret and synthesise. This consequently hampers their ability to inform practice as well as future research and contributes to research waste. Better transparency and adequate reporting will enable the potential benefits of ADs to be realised.This extension to the Consolidated Standards Of Reporting Trials (CONSORT) 2010 statement was developed to enhance the reporting of randomised AD clinical trials. We developed an Adaptive designs CONSORT Extension (ACE) guideline through a two-stage Delphi process with input from multidisciplinary key stakeholders in clinical trials research in the public and private sectors from 21 countries, followed by a consensus meeting. Members of the CONSORT Group were involved during the development process.The paper presents the ACE checklists for AD randomised trial reports and abstracts, as well as an explanation with examples to aid the application of the guideline. The ACE checklist comprises seven new items, nine modified items, six unchanged items for which additional explanatory text clarifies further considerations for ADs, and 20 unchanged items not requiring further explanatory text. The ACE abstract checklist has one new item, one modified item, one unchanged item with additional explanatory text for ADs, and 15 unchanged items not requiring further explanatory text.The intention is to enhance transparency and improve reporting of AD randomised trials to improve the interpretability of their results and reproducibility of their methods, results and inference. We also hope indirectly to facilitate the much-needed knowledge transfer of innovative trial designs to maximise their potential benefits.},
  journal = {BMJ (Clinical research ed.)},
  keywords = {Checklist,Consensus,Delphi Technique,Guidelines as Topic,Humans,Periodicals as Topic,Publishing,Quality Control,Randomized Controlled Trials as Topic,Reproducibility of Results,Research Design},
  language = {eng},
  pmcid = {PMC7298567},
  pmid = {32554564}
}

@article{dodge_method_1929,
  title = {A {{Method}} of {{Sampling Inspection}}},
  author = {Dodge, H. F. and Romig, H. G.},
  year = {1929},
  month = oct,
  volume = {8},
  pages = {613--631},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1929.tb01240.x},
  abstract = {This paper outlines some of the general considerations which must be taken into account in setting up any practical sampling inspection plan. An economical method of inspection is developed in detail for the case where the purpose of the inspection is to determine the acceptability of discrete lots of a product submitted by a producer. By employing probability theory, the method places a definite barrier in the path of material of defective quality and gives this protection to the consumer with a minimum of inspection expense.},
  journal = {Bell System Technical Journal},
  language = {en},
  number = {4}
}

@article{dupont_sequential_1983,
  title = {Sequential Stopping Rules and Sequentially Adjusted {{P}} Values: {{Does}} One Require the Other?},
  shorttitle = {Sequential Stopping Rules and Sequentially Adjusted {{P}} Values},
  author = {Dupont, William D.},
  year = {1983},
  month = jan,
  volume = {4},
  pages = {3--10},
  issn = {0197-2456},
  doi = {10.1016/S0197-2456(83)80003-8},
  abstract = {During the course of a clinical trial it is normally necessary to conduct periodic reviews of the data in order to determine whether the trial should be terminated. Since these reviews affect the probability of the final outcome, many statisticians recommend that the P values quoted for a clinical trial be sequentially adjusted to account for the possibility of premature termination. In this article it is argued that the sequentially adjusted P value is an inappropriate measure of the strength of evidence justified by a clinical trial. This is because the size of sequentially adjusted P values will vary according to actions that might have been taken if the trial had gone differently than it in fact did. Although such contingencies will effect the frequency of occurrence of certain events in hypothetical sequence of trial replications, it is hard to see why decisions that would have been made in response to outcomes that did not occur should have any bearing on the strength of evidence that can be attributed to the results that were actually observed. The credibility merited by a clinical trial depends not only on the implausibility of the observed results under the null hypothesis, but also on factors such as the medical plausibility of hypothesis well supported by the data, and the extent to which observed results have been predicted in advance. It is argued that publishing these factors along with fixed sample P values is the best way to indicate the degree of certainty that should be attributed to the conclusions of a clinical trial.},
  journal = {Controlled Clinical Trials},
  keywords = {Clinical trials,foundations of statistical inference,likelihood principle,Sequential stopping rules},
  language = {en},
  number = {1}
}

@article{erdfelder_gpower_1996,
  title = {{{GPOWER}}: {{A}} General Power Analysis Program},
  shorttitle = {{{GPOWER}}},
  author = {Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  year = {1996},
  month = mar,
  volume = {28},
  pages = {1--11},
  issn = {0743-3808, 1532-5970},
  doi = {10.3758/BF03203630},
  journal = {Behavior Research Methods, Instruments, \& Computers},
  language = {en},
  number = {1}
}

@article{fiedler_questionable_2016,
	title = {Questionable {Research} {Practices} {Revisited}},
	volume = {7},
	issn = {1948-5506},
	url = {https://doi.org/10.1177/1948550615612150},
	doi = {10.1177/1948550615612150},
	abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
	language = {en},
	number = {1},
	urldate = {2021-01-12},
	journal = {Social Psychological and Personality Science},
	author = {Fiedler, Klaus and Schwarz, Norbert},
	month = jan,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	keywords = {ethics/morality, language, research methods, research practices, survey methodology},
	pages = {45--52}
}

@article{gillen_note_2005,
  title = {A {{Note}} on {{P}}-{{Values}} under {{Group Sequential Testing}} and {{Nonproportional Hazards}}},
  author = {Gillen, Daniel L. and Emerson, Scott S.},
  year = {2005},
  volume = {61},
  pages = {546--551},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2005.040342.x},
  abstract = {Group sequential designs are often used for periodically assessing treatment efficacy during the course of a clinical trial. Following a group sequential test, P-values computed under the assumption that the data were gathered according to a fixed sample design are no longer uniformly distributed under the null hypothesis of no treatment effect. Various sample space orderings have been proposed for computing proper P-values following a group sequential test. Although many of the proposed orderings have been compared in the setting of time-invariant treatment effects, little attention has been given to their performance when the effect of treatment within an individual varies over time. Our interest here is to compare two of the most commonly used methods for computing proper P-values following a group sequential test, based upon the analysis time (AT) and Z-statistic orderings, with respect to resulting power functions when treatment effects on survival are delayed. Power under the AT ordering is shown to be heavily influenced by the presence of a delayed treatment effect, while power functions corresponding to the Z-statistic ordering remain robust under time-varying treatment effects.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2005.040342.x},
  journal = {Biometrics},
  keywords = {Censored data,Clinical trials,Nonproportional hazards,Ordering,Sequential tests},
  language = {en},
  number = {2}
}

@book{jennison_group_2000,
  title = {Group Sequential Methods with Applications to Clinical Trials},
  author = {Jennison, Christopher and Turnbull, Bruce W.},
  year = {2000},
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-0-8493-0316-6},
  keywords = {Clinical Trials,Decision Theory,methods,Models; Statistical,Statistical methods,Statistics},
  lccn = {R853.C55 J46 2000}
}

@article{john_measuring_2012,
  title = {Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  volume = {23},
  pages = {524--532},
  doi = {10.1177/0956797611430953},
  annotation = {00795},
  journal = {Psychological science},
  number = {5}
}

@article{lakens_equivalence_2018,
  title = {Equivalence Testing for Psychological Research: {{A}} Tutorial},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  volume = {1},
  pages = {259--269},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@article{lakens_performing_2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  volume = {44},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  journal = {European Journal of Social Psychology},
  language = {en},
  number = {7}
}

@article{lan_discrete_1983,
  title = {Discrete {{Sequential Boundaries}} for {{Clinical Trials}}},
  author = {Lan, K. K. Gordon and DeMets, David L.},
  year = {1983},
  month = dec,
  volume = {70},
  pages = {659--663},
  issn = {00063444},
  doi = {10.2307/2336502},
  annotation = {01888},
  journal = {Biometrika},
  number = {3}
}

@article{mudge_setting_2012,
  title = {Setting an {{Optimal}} {$\alpha$} {{That Minimizes Errors}} in {{Null Hypothesis Significance Tests}}},
  author = {Mudge, Joseph F. and Baker, Leanne F. and Edge, Christopher B. and Houlahan, Jeff E.},
  year = {2012},
  month = feb,
  volume = {7},
  pages = {e32734},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0032734},
  abstract = {Null hypothesis significance testing has been under attack in recent years, partly owing to the arbitrary nature of setting {$\alpha$} (the decision-making threshold and probability of Type I error) at a constant value, usually 0.05. If the goal of null hypothesis testing is to present conclusions in which we have the highest possible confidence, then the only logical decision-making threshold is the value that minimizes the probability (or occasionally, cost) of making errors. Setting {$\alpha$} to minimize the combination of Type I and Type II error at a critical effect size can easily be accomplished for traditional statistical tests by calculating the {$\alpha$} associated with the minimum average of {$\alpha$} and {$\beta$} at the critical effect size. This technique also has the flexibility to incorporate prior probabilities of null and alternate hypotheses and/or relative costs of Type I and Type II errors, if known. Using an optimal {$\alpha$} results in stronger scientific inferences because it estimates and minimizes both Type I errors and relevant Type II errors for a test. It also results in greater transparency concerning assumptions about relevant effect size(s) and the relative costs of Type I and II errors. By contrast, the use of {$\alpha$} = 0.05 results in arbitrary decisions about what effect sizes will likely be considered significant, if real, and results in arbitrary amounts of Type II error for meaningful potential effect sizes. We cannot identify a rationale for continuing to arbitrarily use {$\alpha$} = 0.05 for null hypothesis significance tests in any field, when it is possible to determine an optimal {$\alpha$}.},
  journal = {PLOS ONE},
  keywords = {Agricultural soil science,Decision making,Experimental design,Freshwater fish,Gene expression,Lakes,Research errors,Shores},
  number = {2}
}

@article{obrien_multiple_1979,
	title = {A multiple testing procedure for clinical trials},
	volume = {35},
	doi = {https://doi.org/10.2307/2530245},
	number = {3},
	journal = {Biometrics},
	author = {O'Brien, Peter C. and Fleming, Thomas R.},
	year = {1979},
	note = {Publisher: JSTOR},
	pages = {549--556},
	file = {O'Brien_Fleming_1979_A multiple testing procedure for clinical trials.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\UD4BBBZ5\\O'Brien_Fleming_1979_A multiple testing procedure for clinical trials.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\IMVPHMXY\\2530245.html:text/html}
}

@article{pallmann_adaptive_2018,
  title = {Adaptive Designs in Clinical Trials: Why Use Them, and How to Run and Report Them},
  shorttitle = {Adaptive Designs in Clinical Trials},
  author = {Pallmann, Philip and Bedding, Alun W. and {Choodari-Oskooei}, Babak and Dimairo, Munyaradzi and Flight, Laura and Hampson, Lisa V. and Holmes, Jane and Mander, Adrian P. and Odondi, Lang'o and Sydes, Matthew R. and Villar, Sof{\'i}a S. and Wason, James M. S. and Weir, Christopher J. and Wheeler, Graham M. and Yap, Christina and Jaki, Thomas},
  year = {2018},
  month = feb,
  volume = {16},
  pages = {29},
  issn = {1741-7015},
  doi = {10.1186/s12916-018-1017-7},
  abstract = {Adaptive designs can make clinical trials more flexible by utilising results accumulating in the trial to modify the trial's course in accordance with pre-specified rules. Trials with an adaptive design are often more efficient, informative and ethical than trials with a traditional fixed design since they often make better use of resources such as time and money, and might require fewer participants. Adaptive designs can be applied across all phases of clinical research, from early-phase dose escalation to confirmatory trials. The pace of the uptake of adaptive designs in clinical research, however, has remained well behind that of the statistical literature introducing new methods and highlighting their potential advantages. We speculate that one factor contributing to this is that the full range of adaptations available to trial designs, as well as their goals, advantages and limitations, remains unfamiliar to many parts of the clinical community. Additionally, the term adaptive design has been misleadingly used as an all-encompassing label to refer to certain methods that could be deemed controversial or that have been inadequately implemented.},
  journal = {BMC Medicine},
  number = {1}
}

@article{pocock_group_1977,
	title = {Group sequential methods in the design and analysis of clinical trials},
	volume = {64},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/64.2.191},
	doi = {10.1093/biomet/64.2.191},
	abstract = {In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes be statistically superior to standard sequential designs.},
	number = {2},
	urldate = {2021-01-12},
	journal = {Biometrika},
	author = {Pocock, Stuart J.},
	month = aug,
	year = {1977},
	pages = {191--199},
	file = {Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\B6QTUGJC\\384776.html:text/html}
}

@book{proschan_statistical_2006,
  title = {Statistical Monitoring of Clinical Trials: A Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, K. K. Gordan and Wittes, Janet Turk},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-0-387-30059-7},
  keywords = {Bayes Theorem,Clinical Trials,Data Interpretation; Statistical,Drugs,Statistical methods,Statistics,statistics \& numerical data,Testing},
  lccn = {R853.C55 .P76 2006},
  series = {Statistics for Biology and Health}
}

@article{schoenfeld_procon_2005,
  title = {Pro/Con Clinical Debate: {{It}} Is Acceptable to Stop Large Multicentre Randomized Controlled Trials at Interim Analysis for Futility},
  shorttitle = {Pro/Con Clinical Debate},
  author = {Schoenfeld, David A and Meade, Maureen O},
  year = {2005},
  volume = {9},
  pages = {34--36},
  issn = {1364-8535},
  doi = {10.1186/cc3013},
  abstract = {A few recent, large, well-publicized trials in critical care medicine have been stopped for futility. In the critical care setting, stopping for futility means that independent review committees have elected to stop the trial early \textendash{} based on predetermined rules \textendash{} since the likelihood of finding a treatment effect is low. For bedside clinicians the idea of futility in a clinical trial can be confusing. In the present article, two experts in the conduct of clinical trials debate the role of futility-stopping rules.},
  journal = {Critical Care},
  number = {1},
  pmcid = {PMC1065108},
  pmid = {15693981}
}

@article{schonbrodt_sequential_2017,
  title = {Sequential Hypothesis Testing with {{Bayes}} Factors: {{Efficiently}} Testing Mean Differences},
  shorttitle = {Sequential Hypothesis Testing with {{Bayes}} Factors},
  author = {Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
  year = {2017},
  month = jun,
  volume = {22},
  pages = {322--339},
  issn = {1939-1463},
  doi = {10.1037/MET0000061},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference. (PsycINFO Database Record},
  annotation = {00070},
  journal = {Psychological Methods},
  keywords = {Bayes Theorem,Data Interpretation; Statistical,Humans,Probability,Research Design,Sample Size},
  language = {eng},
  number = {2},
  pmid = {26651986}
}

@article{spiegelhalter_monitoring_1986,
	title = {Monitoring clinical trials: conditional or predictive power?},
	volume = {7},
	shorttitle = {Monitoring clinical trials},
	doi = {https://doi.org/10.1016/0197-2456(86)90003-6},
	number = {1},
	journal = {Controlled clinical trials},
	author = {Spiegelhalter, David J. and Freedman, Laurence S. and Blackburn, Patrick R.},
	year = {1986},
	note = {Publisher: Elsevier},
	pages = {8--17},
	file = {Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\5D63XDDV\\0197245686900036.html:text/html}
}

@article{stefan_tutorial_2019,
  title = {A Tutorial on {{Bayes Factor Design Analysis}} Using an Informed Prior},
  author = {Stefan, Angelika M. and Gronau, Quentin F. and Sch{\"o}nbrodt, Felix D. and Wagenmakers, Eric-Jan},
  year = {2019},
  month = jun,
  volume = {51},
  pages = {1042--1058},
  issn = {1554-3528},
  doi = {10.3758/s13428-018-01189-8},
  abstract = {Well-designed experiments are likely to yield compelling evidence with efficient sample sizes. Bayes Factor Design Analysis (BFDA) is a recently developed methodology that allows researchers to balance the informativeness and efficiency of their experiment (Sch\"onbrodt \& Wagenmakers, Psychonomic Bulletin \& Review, 25(1), 128\textendash 142 2018). With BFDA, researchers can control the rate of misleading evidence but, in addition, they can plan for a target strength of evidence. BFDA can be applied to fixed-N and sequential designs. In this tutorial paper, we provide an introduction to BFDA and analyze how the use of informed prior distributions affects the results of the BFDA. We also present a user-friendly web-based BFDA application that allows researchers to conduct BFDAs with ease. Two practical examples highlight how researchers can use a BFDA to plan for informative and efficient research designs.},
  journal = {Behavior Research Methods},
  language = {en},
  number = {3}
}

@article{wald_sequential_1945,
  title = {Sequential Tests of Statistical Hypotheses},
  author = {Wald, Abraham},
  year = {1945},
  volume = {16},
  pages = {117--186},
  journal = {The Annals of Mathematical Statistics},
  number = {2}
}

@article{wang_approximately_1987,
	title = {Approximately optimal one-parameter boundaries for group sequential trials},
	volume = {43},
	doi = {https://doi.org/10.2307/2531959},
	number = {1},
	journal = {Biometrics},
	author = {Wang, Samuel K. and Tsiatis, Anastasios A.},
	year = {1987},
	note = {Publisher: JSTOR},
	pages = {193--199},
	file = {Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\FUXXLYHV\\2531959.html:text/html;Wang_Tsiatis_1987_Approximately optimal one-parameter boundaries for group sequential trials.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\J9HSUK3I\\Wang_Tsiatis_1987_Approximately optimal one-parameter boundaries for group sequential trials.pdf:application/pdf}
}

@book{wassmer_group_2016,
  title = {Group {{Sequential}} and {{Confirmatory Adaptive Designs}} in {{Clinical Trials}}},
  author = {Wassmer, Gernot and Brannath, Werner},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-32562-0},
  isbn = {978-3-319-32560-6 978-3-319-32562-0},
  language = {en},
  series = {Springer {{Series}} in {{Pharmaceutical Statistics}}}
}

@misc{wassmer_rpact_2019,
  title = {{{rpact}}: {{Confirmatory}} {{Adaptive Clinical Trial Design}} and {{Analysis}}},
  author = {Wassmer, Gernot and Pahlke, Friedrich},
  year = {2020},
  note = {R package version 3.0.3},
  url = {https://www.rpact.org}
}


@techreport{lakens_will_2017,
	title = {Will knowledge about more efficient study designs increase the willingness to pre-register?},
	url = {https://osf.io/preprints/metaarxiv/svzyc/},
	abstract = {Pre-registration is a straightforward way to make science more transparant, and control Type 1 error rates. Pre-registration is often presented as beneficial for science in general, but rarely  as a practice that leads to immediate individual benefits for researchers. One benefit of pre-registered studies is that  they allow for non-conventional research designs that are more efficient than conventional  designs. For example, by performing one-tailed tests and sequential analyses researchers  can perform well-powered studies much more efficiently. Here, I examine whether such non-conventional but more efficient designs are considered appropriate by editors under the pre-condition that the analysis plans are pre-registered, and if so, whether researchers are more willing to pre-register their analysis plan to take advantage of the efficiency benefits of non-conventional designs. Study 1 shows the large majority of editors judged one-tailed tests and sequential analyses to be appropriate in psychology, but only when such analyses are pre-registered. In Study 2 I asked experimental psychologists to indicate their attitude towards pre-registration. Half of these researchers first read about the acceptence of one-tailed tests and sequential analyses by editors, and the efficiency gains of using these procedures. However, learning about the efficiency benefits associated with one-tailed tests  and sequential analyses did not substantially influence researchers' attitudes about benefits and costs of pre-registration, or their willingness to pre-register studies. The self-reported likelihood of pre-registering studies in the next two years, as  well as the percentage of studies researchers planned to pre-register in the future, was surprisingly high. 47\% of respondents already had experience pre-registering, and 94\% of respondents indicating that they would consider pre-registering at least some of their research in the future. Given this already strong self-reported willingness to pre-register studies, pointing out immediate individual benefits seems unlikely to be a useful way to increase researchers' willingness to pre-register any further.},
	urldate = {2020-12-06},
	institution = {MetaArXiv},
	author = {Lakens, Daniel},
	month = mar,
	year = {2017},
	doi = {10.31222/osf.io/svzyc},
	keywords = {Meta-science, One-tailed Tests, Pre-registration, Psychology, Sequential Analysis, Social and Behavioral Sciences, Social Statistics},
	file = {Lakens_2017_Will knowledge about more efficient study designs increase the willingness to.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\BINZYZIN\\Lakens_2017_Will knowledge about more efficient study designs increase the willingness to.pdf:application/pdf}
}

@article{lakens_value_2019,
	title = {The value of preregistration for psychological science: {A} conceptual analysis},
	shorttitle = {The {Value} of {Preregistration} for {Psychological} {Science}},
	url = {https://psyarxiv.com/jbh4w/},
	doi = {https://doi.org/10.24602/sjpr.62.3_221},
	abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
	number = {62(3)},
	urldate = {2019-11-19},
	journal = {Japanese Psychological Review},
	author = {Lakens, Daniel},
	year = {2019},
	pages = {221--230},
	file = {Lakens_2019_The Value of Preregistration for Psychological Science.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\VG9M7TLI\\Lakens_2019_The Value of Preregistration for Psychological Science.pdf:application/pdf;Lakens_JPR62(3)221-230.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\CH48CJIX\\Lakens_JPR62(3)221-230.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\7RIXC7RC\\jbh4w.html:text/html}
}

@Manual{shiny_app_2020,
  title = {{{shiny}}: Web Application Framework for R},
  author = {Winston Chang and Joe Cheng and JJ Allaire and Yihui Xie and Jonathan McPherson},
  year = {2020},
  note = {R package version 1.5.0},
  url = {https://CRAN.R-project.org/package=shiny},
}

@Manual{r_2020,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2020},
  url = {https://www.R-project.org/},
}


@article{appelbaum_journal_2018,
	title = {Journal article reporting standards for quantitative research in psychology: {The} {APA} {Publications} and {Communications} {Board} task force report.},
	volume = {73},
	issn = {1935-990X},
	shorttitle = {Journal article reporting standards for quantitative research in psychology},
	url = {https://psycnet.apa.org/fulltext/2018-00750-002.pdf},
	doi = {10.1037/amp0000191},
	number = {1},
	urldate = {2020-07-25},
	journal = {American Psychologist},
	author = {Appelbaum, Mark and Cooper, Harris and Kline, Rex B. and Mayo-Wilson, Evan and Nezu, Arthur M. and Rao, Stephen M.},
	year = {2018},
	note = {Publisher: US: American Psychological Association},
	pages = {3--25}
}


@article{schnuerch_controlling_2020,
	title = {Controlling decision errors with minimal costs: {The} sequential probability ratio t test.},
	volume = {25},
	shorttitle = {Controlling decision errors with minimal costs},
	number = {2},
	journal = {Psychological methods},
	author = {Schnuerch, Martin and Erdfelder, Edgar},
	year = {2020},
	note = {Publisher: American Psychological Association},
	pages = {206},
	file = {Schnuerch_Erdfelder_2020_Controlling decision errors with minimal costs.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\9CMCDN5D\\Schnuerch_Erdfelder_2020_Controlling decision errors with minimal costs.pdf:application/pdf;Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\3DAA8JKW\\2019-52380-001.html:text/html}
}


@article{miller_simple_2021,
	title = {A simple, general, and efficient procedure for sequential hypothesis testing: {The} independent segments procedure},
	abstract = {We propose a new sequential hypothesis testing procedure in which data are collected and analyzed in a series of independent segments. As in fixed-sample hypothesis testing and in previous sequential procedures, the overall alpha level can be set to any desired value. Like other sequential procedures, the independent segments procedure generally requires smaller samples than fixed-sample procedures—often approximately 30\% smaller—to achieve the same alpha level and statistical power. Relative to other sequential procedures, the new method has the advantages that it is simpler to use, requires fewer assumptions, and can be used with a wider array of statistical tests. Thus, in some circumstances the independent segments procedure may provide an attractive option for increasing the efficiency of statistical testing.},
	journal = {Psychological Methods},
	author = {Miller, Jeff and Ulrich, Rolf},
	year = {2021}
}


@article{westberg_combining_1985,
	title = {Combining {Independent} {Statistical} {Tests}},
	volume = {34},
	issn = {0039-0526},
	url = {https://www.jstor.org/stable/2987655},
	doi = {10.2307/2987655},
	abstract = {In the present study two well-known combination methods, Fisher's and Tippett's, are compared according to their power. The calculations are made for normally and chi-square distributed test statistics. None of the two procedures is uniformly better than the other according to the power but sometimes the power curves cross each other. The calculated power-graphs give guidelines for when to use Fisher's method and when to use Tippett's.},
	number = {3},
	urldate = {2020-12-30},
	journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
	author = {Westberg, Margareta},
	year = {1985},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {287--296},
	file = {Westberg_1985_Combining Independent Statistical Tests.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\RZ76ZEUS\\Westberg_1985_Combining Independent Statistical Tests.pdf:application/pdf}
}

@article{grunwald_safe_2019,
	title = {Safe {Testing}},
	url = {http://arxiv.org/abs/1906.07801},
	abstract = {We present a new theory of hypothesis testing. The main concept is the S-value, a notion of evidence which, unlike p-values, allows for effortlessly combining evidence from several tests, even in the common scenario where the decision to perform a new test depends on the previous test outcome: safe tests based on S-values generally preserve Type-I error guarantees under such "optional continuation". S-values exist for completely general testing problems with composite null and alternatives. Their prime interpretation is in terms of gambling or investing, each S-value corresponding to a particular investment. Surprisingly, optimal "GROW" S-values, which lead to fastest capital growth, are fully characterized by the joint information projection (JIPr) between the set of all Bayes marginal distributions on H0 and H1. Thus, optimal S-values also have an interpretation as Bayes factors, with priors given by the JIPr. We illustrate the theory using two classical testing scenarios: the one-sample t-test and the 2x2 contingency table. In the t-test setting, GROW s-values correspond to adopting the right Haar prior on the variance, like in Jeffreys' Bayesian t-test. However, unlike Jeffreys', the "default" safe t-test puts a discrete 2-point prior on the effect size, leading to better behavior in terms of statistical power. Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, S-values and safe tests may provide a methodology acceptable to adherents of all three schools.},
	urldate = {2019-10-21},
	journal = {arXiv:1906.07801 [cs, math, stat]},
	author = {Grünwald, Peter and de Heide, Rianne and Koolen, Wouter},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.07801},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory, Computer Science - Information Theory, Computer Science - Machine Learning},
	annote = {Comment: Preliminary version, not yet submitted to a journal},
	file = {arXiv.org Snapshot:C\:\\Users\\dlakens\\Zotero\\storage\\MJANG9J2\\1906.html:text/html;Grünwald et al. - 2019 - Safe Testing.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\BWT7DIIA\\Grünwald et al. - 2019 - Safe Testing.pdf:application/pdf}
}

@article{ter_schure_accumulation_2019,
	title = {Accumulation {Bias} in meta-analysis: the need to consider time in error control},
	volume = {8},
	issn = {2046-1402},
	shorttitle = {Accumulation {Bias} in meta-analysis},
	url = {https://f1000research.com/articles/8-962/v1},
	doi = {10.12688/f1000research.19375.1},
	language = {en},
	urldate = {2020-12-27},
	journal = {F1000Research},
	author = {ter Schure, Judith and Grünwald, Peter},
	month = jun,
	year = {2019},
	pages = {962},
	file = {Accumulation Bias in meta-analysis\: the need to... | F1000Research:C\:\\Users\\dlakens\\Zotero\\storage\\H29EI8PU\\v1.html:text/html;ter Schure_Grünwald_2019_Accumulation Bias in meta-analysis.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\3FWXEKXB\\ter Schure_Grünwald_2019_Accumulation Bias in meta-analysis.pdf:application/pdf}
}


@article{frick_better_1998,
	title = {A better stopping rule for conventional statistical tests},
	volume = {30},
	url = {http://link.springer.com/article/10.3758/BF03209488},
	doi = {https://doi.org/10.3758/BF03209488},
	number = {4},
	urldate = {2015-11-30},
	journal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Frick, Robert W.},
	year = {1998},
	pages = {690--697},
	file = {Frick - COAST sequential analyses.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\HFF73HR5\\Frick - COAST sequential analyses.pdf:application/pdf}
}


@article{mayo_how_2008,
	title = {How to {Discount} {Double}-{Counting} {When} {It} {Counts}: {Some} {Clarifications}},
	volume = {59},
	issn = {0007-0882, 1464-3537},
	shorttitle = {How to {Discount} {Double}-{Counting} {When} {It} {Counts}},
	url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axn034},
	doi = {10.1093/bjps/axn034},
	abstract = {The issues of double-counting, use-constructing, and selection effects have long be the subject of debate in the philosophical as well as statistical literature. I have argu that it is the severity, stringency, or probativeness of the test - or lack of it - that sho determine if a double-use of data is admissible. Hitchcock and Sober ([2004]) questio whether this 'severity criterion' can perform its intended job. I argue that their criticis stem from a flawed interpretation of the severity criterion. Taking their criticism springboard, I elucidate some of the central examples that have long been controvers and clarify how the severity criterion is properly applied to them.},
	language = {en},
	number = {4},
	urldate = {2021-01-12},
	journal = {The British Journal for the Philosophy of Science},
	author = {Mayo, Deborah G.},
	month = dec,
	year = {2008},
	pages = {857--879},
	file = {Mayo - 2008 - How to Discount Double-Counting When It Counts So.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\3CHHQ89Y\\Mayo - 2008 - How to Discount Double-Counting When It Counts So.pdf:application/pdf}
}


@techreport{lakens_sample_2021,
	title = {Sample {Size} {Justification}},
	url = {https://psyarxiv.com/9d3yf/},
	abstract = {An important step when designing a study is to justify the sample size that will be collected. The key aim of a sample size justification is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (an)almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are plausible in a specific research area. Researchers can use the guidelines presented in this article to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
	urldate = {2021-01-04},
	institution = {PsyArXiv},
	author = {Lakens, Daniel},
	month = jan,
	year = {2021},
	doi = {10.31234/osf.io/9d3yf},
	note = {type: article},
	keywords = {power analysis, value of information, Experimental Design and Sample Surveys, Quantitative Methods, Social and Behavioral Sciences, sample size justification, study design},
	file = {Lakens_2021_Sample Size Justification.pdf:C\:\\Users\\dlakens\\Zotero\\storage\\ENBKIT5E\\Lakens_2021_Sample Size Justification.pdf:application/pdf}
}
